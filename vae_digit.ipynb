{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Variational Autoencoder (VAE) with the Keras Functional API.\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & model configuration\n",
    "img_width, img_height = input_train.shape[1], input_train.shape[2]\n",
    "batch_size = 200\n",
    "no_epochs = 50\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "latent_dim = 32\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data\n",
    "input_train = input_train.reshape(input_train.shape[0], img_height, img_width, num_channels)\n",
    "input_test = input_test.reshape(input_test.shape[0], img_height, img_width, num_channels)\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =================\n",
    "# # Encoder\n",
    "# # =================\n",
    "\n",
    "# Definition\n",
    "i       = Input(shape=input_shape, name='encoder_input')\n",
    "cx      = Conv2D(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(i)\n",
    "cx      = BatchNormalization()(cx)\n",
    "cx      = Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\n",
    "cx      = BatchNormalization()(cx)\n",
    "x       = Flatten()(cx)\n",
    "x       = Dense(20, activation='relu')(x)\n",
    "x       = BatchNormalization()(x)\n",
    "mu      = Dense(latent_dim, name='latent_mu')(x)\n",
    "sigma   = Dense(latent_dim, name='latent_sigma')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7, 7, 16)\n"
     ]
    }
   ],
   "source": [
    "# Get Conv2D shape for Conv2DTranspose operation in decoder\n",
    "conv_shape = K.int_shape(cx)\n",
    "print(conv_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling with reparameterization trick\n",
    "def sample_z(args):\n",
    "  mu, sigma = args\n",
    "  batch     = K.shape(mu)[0]\n",
    "  dim       = K.int_shape(mu)[1]\n",
    "  eps       = K.random_normal(shape=(batch, dim))\n",
    "  return mu + K.exp(sigma / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reparameterization trick to ensure correct gradient\n",
    "z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 8)    80          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 8)    32          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 7, 7, 16)     1168        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 7, 7, 16)     64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 784)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 20)           15700       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20)           80          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "latent_mu (Dense)               (None, 32)           672         batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "latent_sigma (Dense)            (None, 32)           672         batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 32)           0           latent_mu[0][0]                  \n",
      "                                                                 latent_sigma[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 18,468\n",
      "Trainable params: 18,380\n",
      "Non-trainable params: 88\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate encoder\n",
    "encoder = Model(i, [mu, sigma, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# Decoder\n",
    "# =================\n",
    "\n",
    "# Definition\n",
    "d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "x     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\n",
    "x     = BatchNormalization()(x)\n",
    "x     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "cx    = Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
    "cx    = BatchNormalization()(cx)\n",
    "cx    = Conv2DTranspose(filters=8, kernel_size=3, strides=2, padding='same',  activation='relu')(cx)\n",
    "cx    = BatchNormalization()(cx)\n",
    "o     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 784)               25872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 28, 28, 8)         32        \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 28, 28, 1)         73        \n",
      "=================================================================\n",
      "Total params: 32,657\n",
      "Trainable params: 31,041\n",
      "Non-trainable params: 1,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate decoder\n",
    "decoder = Model(d_i, o, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 32), (None, 32),  18468     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         32657     \n",
      "=================================================================\n",
      "Total params: 51,125\n",
      "Trainable params: 49,421\n",
      "Non-trainable params: 1,704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# VAE as a whole\n",
    "# =================\n",
    "\n",
    "# Instantiate VAE\n",
    "vae_outputs = decoder(encoder(i)[2])\n",
    "vae         = Model(i, vae_outputs, name='vae')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss \n",
    "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= image_size * image_size\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "def kl_reconstruction_loss(true, pred):\n",
    "  # Reconstruction loss\n",
    "  reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n",
    "  # KL divergence loss\n",
    "  kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "  kl_loss = K.sum(kl_loss, axis=-1)\n",
    "  kl_loss *= -0.5\n",
    "  # Total loss = 50% rec + 50% KL divergence loss\n",
    "  return K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 21s 431us/step - loss: 3424.8876 - val_loss: 3485.4847\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 20s 411us/step - loss: 3452.6871 - val_loss: 3485.3539\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.6482 - val_loss: 3485.3301\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.6328 - val_loss: 3485.3173\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.6244 - val_loss: 3485.3099\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 18s 378us/step - loss: 3452.6193 - val_loss: 3485.3054\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.6155 - val_loss: 3485.3019\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 18s 383us/step - loss: 3452.6129 - val_loss: 3485.2996\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 18s 385us/step - loss: 3452.6110 - val_loss: 3485.2978\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 19s 386us/step - loss: 3452.6094 - val_loss: 3485.2964\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 19s 386us/step - loss: 3452.6083 - val_loss: 3485.2952\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 19s 385us/step - loss: 3452.6072 - val_loss: 3485.2942\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 19s 388us/step - loss: 3452.6064 - val_loss: 3485.2933\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 19s 388us/step - loss: 3452.6058 - val_loss: 3485.2926\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 18s 383us/step - loss: 3452.6051 - val_loss: 3485.2922\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 18s 381us/step - loss: 3452.6045 - val_loss: 3485.2916\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 19s 388us/step - loss: 3452.6041 - val_loss: 3485.2912\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6036 - val_loss: 3485.2909\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6033 - val_loss: 3485.2904\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6030 - val_loss: 3485.2900\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6027 - val_loss: 3485.2897\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6024 - val_loss: 3485.2895\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6021 - val_loss: 3485.2893\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 18s 374us/step - loss: 3452.6020 - val_loss: 3485.2892\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 18s 376us/step - loss: 3452.6018 - val_loss: 3485.2890\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 18s 375us/step - loss: 3452.6015 - val_loss: 3485.2888\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 18s 373us/step - loss: 3452.6014 - val_loss: 3485.2886\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 18s 376us/step - loss: 3452.6012 - val_loss: 3485.2884\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6011 - val_loss: 3485.2881\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6009 - val_loss: 3485.2880\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6008 - val_loss: 3485.2880\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 18s 378us/step - loss: 3452.6007 - val_loss: 3485.2879\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 18s 378us/step - loss: 3452.6006 - val_loss: 3485.2878\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6005 - val_loss: 3485.2877\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6004 - val_loss: 3485.2876\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6003 - val_loss: 3485.2875\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 18s 377us/step - loss: 3452.6002 - val_loss: 3485.2875\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 18s 378us/step - loss: 3452.6001 - val_loss: 3485.2874\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 19s 391us/step - loss: 3452.6000 - val_loss: 3485.2873\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 19s 389us/step - loss: 3452.6000 - val_loss: 3485.2873\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 19s 392us/step - loss: 3452.6000 - val_loss: 3485.2872\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 3452.5999 - val_loss: 3485.2871\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 3452.5998 - val_loss: 3485.2871\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.5997 - val_loss: 3485.2870\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 3452.5997 - val_loss: 3485.2869\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 3452.5997 - val_loss: 3485.2868\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.5996 - val_loss: 3485.2867\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 18s 380us/step - loss: 3452.5995 - val_loss: 3485.2867\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 18s 379us/step - loss: 3452.5995 - val_loss: 3485.2866\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 18s 378us/step - loss: 3452.5994 - val_loss: 3485.2866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x642d80350>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile VAE\n",
    "vae.compile(optimizer='sgd', loss=kl_reconstruction_loss)\n",
    "\n",
    "# Train autoencoder\n",
    "vae.fit(input_train, input_train, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Take a sample for visualization purposes\n",
    "# =============================================\n",
    "input_sample = input_test[:1]\n",
    "reconstruction = vae.predict([input_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Visualize input-->reconstruction\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "plt.gray\n",
    "fig.set_size_inches(6, 3.5)\n",
    "input_sample_reshaped = input_sample.reshape((28, 28))\n",
    "reconsstruction_reshaped = reconstruction.reshape((28, 28))\n",
    "axes[0].imshow(input_sample_reshaped) \n",
    "axes[0].set_title('Original image')\n",
    "axes[1].imshow(reconsstruction_reshaped)\n",
    "axes[1].set_title('Reconstruction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x63eb29550>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(reconsstruction_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'builtin_function_or_method' and 'builtin_function_or_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a4e2160cc438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'builtin_function_or_method' and 'builtin_function_or_method'"
     ]
    }
   ],
   "source": [
    "latent = np.random.rand(32)*(max - min) + min\n",
    "decoded_img = decoder.predict(np.array([latent]))\n",
    "decoded_img = decoded_img[0].reshape(28, 28)\n",
    "print(decoded_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
